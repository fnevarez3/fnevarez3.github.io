---
title: 'Project 1: Exploratory Data Analysis'
author: "Frankie Nevarez fgn78"
date: '4/4/21'
output:
  html_document:
    toc: yes
    toc_float:
      collapsed: no
      smooth_scroll: yes
  pdf_document:
    toc: no
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, eval = TRUE, fig.align = "center", warning = F, message = F,
tidy=TRUE, tidy.opts=list(width.cutoff=60), R.options=list(max.print=100))
```



#### 0. Introduction (5  pts)

- Write a narrative introductory paragraph or two describing the datasets you have chosen, the variables they contain, how they were acquired, and why they are interesting to you. Expand on potential associations you may expect, if any.


The datasets I have chosen are from the package fivethirtyeight.  One of the datasets carries the statistics on the different regions and divisions in the United States and the other dataset compares hate crime to income inequality for each state.  Variables include unemployment rate, median household income, white and non white residents, and annual hate crime rate.  These are interesting datasets as hate crimes continue to be seen throughout the US and I wanted to see if there was any links with any of the datasets including region, division, and income inequality.  I predict there will be more hatecrimes in regions with higher minorities just due to more possibility.


#### 1. Tidying: Rearranging Wide/Long (10 pts)

- Tidy the datasets (using the `tidyr` functions `pivot_longer`/`gather` and/or `pivot_wider`/`spread`) 
- If you data sets are already tidy, be sure to use those functions somewhere else in your project (e.g., for rearranging summary statistics)
- Document the process (describe in words what was done)


```{R}
library(tidyverse)
library(fivethirtyeight)
#already tidy. Used 'pivot_longer' on question 4.
```


#### 2. Joining/Merging (10 pts)

- Join your datasets into one using a `dplyr` join function
- If you have multiple observations on the joining variable in either dataset, fix this by collapsing via summarize
- Discuss the process in words, including why you chose the join you did
- Discuss which cases were dropped, if any, and potential problems with this


```{R}
joineddata <- full_join(hate_crimes,state_info,by="state")
head(joineddata)
```
Each of my datasets had the same number of rows because each of the datasets is by state totaling 51 observations for each (one extra for Distric of Columbia). Nothing was dropped in the 'full_join' as the ID variables matched perfectly.  Since the IDs (rows) matched, all the joins would have looked the same and no rows would have been dropped in any of the joins.

#### 3. Wrangling (40 pts)

- Use all six core `dplyr` functions in the service of generating summary tables/statistics (12 pts)
    - Use mutate at least once to generate a variable that is a function of at least one other variable

- Compute summary statistics for each of your variables using `summarize` alone and with `group_by` (if you have more than 10 variables, fine to just focus on 10) (20 pts)
    - Use at least 5 unique functions inside of summarize (e.g., mean, sd)
    - For at least 2, use summarize after grouping by a categorical variable. Create one by dichotomizing a numeric if necessary
    - If applicable, at least 1 of these should group by two categorical variables

- Summarize the procedure and discuss all (or the most interesting) results in no more than two paragraphs (8 pts)


```{R}
joineddata %>% filter(share_non_white>=0.25) %>% arrange(avg_hatecrimes_per_100k_fbi) %>% 
  select(-2,-7,-9,-14,-11,-12) %>% mutate(income_crime = median_house_inc/avg_hatecrimes_per_100k_fbi) -> diverse
head(diverse)

diverse %>% group_by(region) %>% summarize(hatecrime=mean(avg_hatecrimes_per_100k_fbi,na.rm = T),sd(avg_hatecrimes_per_100k_fbi,na.rm=T),min(avg_hatecrimes_per_100k_fbi,na.rm = T),max(avg_hatecrimes_per_100k_fbi,na.rm = T))

diverse %>% group_by(region,division) %>% summarize(median(avg_hatecrimes_per_100k_fbi,na.rm = T))

```
It was interesting to find out that the region with the most annual hatecrimes was the Norteast on average.  The South was also very interesting as it has the state with the least amount of annual hatecrimes, Georgia, and the territory with the most, District of Columbia (DC).  DC is an outlier though because it is greater than 3 standard deviations from the mean which is likely due to its dense political area but small geographic area.  I also found that states in the Mid Atlantic division (NY,PA,NJ) experienced the most hate crime annually while states in the West South Central (AR,LA,OK,TX) experienced the least.

#### 4. Visualizing (30 pts)

- Create a correlation heatmap of your numeric variables the way we did in class

- Create two more effective, polished plots with ggplot

    - Each plot should map 3+ variables to aesthetics 
    - Each plot should have a title and clean labeling for all mappings
    - Change at least one default theme element and color for at least one mapping per plot
    - For at least one plot, add more tick marks (x, y, or both) than are given by default
    - For at least one plot, use the stat="summary" function
    - Supporting paragraph or two (for each plot) describing the relationships/trends that are apparent


```{R}

cormat <- joineddata %>% select_if(is.numeric) %>% cor(use="pair")

tidycor <- cormat %>% as.data.frame %>% rownames_to_column("var1") %>%
pivot_longer(-1,names_to="var2",values_to="correlation")

tidycor%>%ggplot(aes(var1,var2,fill=correlation))+
geom_tile()+
scale_fill_gradient2(low="red",mid="white",high="blue")+
geom_text(aes(label=round(correlation,2)),color = "black", size = 2)+
theme(axis.text.x = element_text(angle = 90, hjust=1))+ 
coord_fixed()


```


```{R}
ggplot(joineddata, aes(share_non_citizen,share_pop_metro )) + geom_point(aes(color=region))+geom_smooth(method="lm")+scale_y_continuous(breaks=seq(0,1.5,.25))+
  ggtitle("Metro v Non US Citizen") + ylab("Metropolitan %") + xlab("Non US Citizen")
```

In this graph, I wanted to compare two of the variables with the highest correlation.  The variables 'share_pop_metro' and 'share_non_citizen' had a correlation coefficient of 0.75.  The implications are that non US citizens tend to live more in metropolitan areas.  The more non US citizens there are in a state, the higher the share of the state population that lives in metropolitan areas.

```{R}
ggplot(joineddata, aes(x = region,y=avg_hatecrimes_per_100k_fbi,fill=division))+
geom_bar(stat="summary", fun=mean)+
  ggtitle("Hate Crimes by Region and Division") + ylab("Hate crimes/100k population") + xlab("Region")

```

This grouped bar chart shows the total number of hate crimes per 100,000 people in each region and division.  This is different than the the previous means taken by region in question 3 because instead of averaging out the regions by division, it adds the means of each division to get the totals for region.  For example, the South region has 3 divisions that are added together which makes it the largest but if the three divisions were averaged out it would have the least average hate crimes per 100,000 people as seen in question 3.  This is an interesting graph comparing the totals for each of the regions and divisions for annual hate crimes.

#### 5. Dimensionality Reduction (30 pts) 

- Either k-means/PAM clustering or PCA (inclusive "or") should be performed on at least three of your variables (3 is just the minimum: using more/all of them will make this much more interesting!)

    - All relevant steps discussed in class (e.g., picking number of PCs/clusters)
    - A visualization of the clusters or the first few principal components (using ggplot2)
    - Supporting paragraph or two describing results found, interpreting the clusters/PCs in terms of the original variables and observations, discussing goodness of fit or variance explained, etc.


```{R}
project1 <- joineddata %>% select(3,10,13) %>% slice(-12)


kmeans1 <- project1 %>% scale %>% kmeans(3)
kmeans1
 
kmeansclust <- project1 %>% mutate(cluster=as.factor(kmeans1$cluster))
kmeansclust %>% ggplot(aes(share_non_white,avg_hatecrimes_per_100k_fbi,median_house_inc, color=cluster)) + geom_point() + ggtitle("Kmeans Cluster") + ylab("Avg Hate Crimes") + xlab("Non White Share")

```
In this graph, I found the kmeans cluster for my dataset using the variables 'share_non_white','avg_hatecrimes_per_100k_fbi', and 'median_house_inc'.  The reason I chose these variables is because I wanted to see how these variables grouped together.  It was pretty tough finding accurate clusters as the variables were not strong correlations with one another as shown by the heatmat above.

#### 6. Neatness, Holistic/Discretionary Points (5 pts)

- Keep things looking nice! Your project should not knit to more than 30 or so pages (probably closer to 10-20)! You will lose points if you print out your entire dataset(s), have terrible formatting, etc. If you start your project in a fresh .Rmd file, you are advised to copy set-up code below and include it: this will do things like automatically truncate if you accidentally print out a huge dataset, etc. Imagine this is a polished report you are giving to your PI or boss to summarize your work researching a topic.



